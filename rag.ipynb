{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cbf3967",
   "metadata": {},
   "source": [
    "## Getting started \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### make document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c637b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c18d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: ./rag_data/Release_2649.docx with loader: UnstructuredWordDocumentLoader\n",
      "Loading file: ./rag_data/Marrapu_Ramlokesh_resume.pdf with loader: PyPDFLoader\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='Date of version: 14.05.2025 Created by: Marrapu.Ramlokesh, Siddharth2.S Confidentiality level: Internal use\\n\\nChange history\\n\\nDate Version Created by Description of change DD.MM.YYYY\\n\\nIntroduction\\n\\nBusiness/Use Case Requirement Brief\\n\\nTo identify AirFiber and Fiber\\xa0customers at risk of churning and enriching the output with additional actionable insights (indicators of problems each customer might have faced). The aim is to enable proactive measures to retain customers and reduce churn.\\n\\nBusiness Description of Change\\n\\nWe are developing a model to predict customer churn probability using multiple data points, including payment history, complaints, outages, and network issues. Customers will be sorted on every billing date based on their likelihood of churn (i.e., probability of churning). This segmentation will allow stakeholders to prioritize interventions and retention strategies.\\n\\nBusiness Usecase\\n\\nJioOne||Campaigns||Retention and Termination||Preventive Churn ML||KPIs\\n\\nBusiness Benefits\\n\\nEnhanced Customer Retention: Proactively identifying at-risk customers and resolving their issues will improve satisfaction and loyalty, reducing overall churn.\\n\\nImproved Revenue: By focusing on customers at high risk of churn and retaining them, the business safeguards recurring revenue streams.\\n\\nData-Driven Decisions: Providing actionable insights into customer health allows for targeted campaigns and better service prioritization.\\n\\nCustomer Experience Improvement: Prioritizing service restoration and addressing complaints for at-risk customers improves their overall experience and perception of Jio services.\\n\\nList of KPIs\\n\\nconnectivity_home.preventive_churn_ml_moat_po\\n\\nconnectivity_home.preventive_churn_ml_model_inference_po\\n\\nconnectivity_home.preventive_churn_ml_campaign_po\\n\\nQueue Analysis\\n\\nQueue Name DAG name Table name Driver_mem Driver cores Exec_mem Executor cores driver mem overhead exec mem overhead Max_executors Number of containers used time taken in min %queue_used batch-jobs churn_feature_set_2_daily preventive_churn_ml_sub6_acqu_o 1G 1 3G 3 1G 1G 3 10 5.21 0.02 batch-jobs churn_feature_set_2_daily preventive_churn_ml_fiber_acqu_o 1G 1 3G 3 1G 1G 3 10 4.80 0.02 batch-jobs churn_feature_set_2_daily preventive_churn_ml_ubr_acqu_o 1G 1 3G 3 1G 1G 3 10 3.44 0.01 batch-jobs churn_feature_set_2_daily preventive_churn_ml_hsi_po 2G 1 8G 5 1G 1G 10 51 44.52 0.08 batch-jobs churn_feature_set_2_daily preventive_churn_ml_stb_po 3G 1 22G 5 1G 2G 22 111 29.5 0.47 batch-jobs churn_feature_set_2_daily preventive_churn_ml_care_po 2G 1 8G 5 1G 2G 10 51 42.78 0.08 batch-jobs churn_feature_set_2_daily preventive_churn_ml_outages_po 2G 1 8G 5 1G 1G 10 51 14.82 0.07 batch-jobs churn_feature_set_2_daily preventive_churn_ml_workorder_po 2G 1 8G 5 1G 1G 10 51 10.25 0.06 batch-jobs churn_feature_set_3_daily preventive_churn_ml_paymentsfeature_o 1G 1 1G 3 1G 1G 1 4 1.43 0.00 batch-jobs churn_feature_set_1_daily preventive_churn_ml_ont_daily_agg_po 2G 1 8G 5 1G 1G 10 51 15.40 0.08 batch-jobs churn_feature_set_1_daily preventive_churn_ml_ont_po 1G 1 3G 5 1G 1G 5 26 9.10 0.05 batch-jobs churn_feature_set_1_daily preventive_churn_ml_alarms_po 2G 1 10G 5 1G 2G 24 121 88.00 0.24 batch-jobs churn_feature_set_3_daily preventive_churn_ml_moat_po 2G 1 5G 5 1G 1G 10 51 5.63 0.06 batch-jobs churn_feature_set_3_daily preventive_churn_ml_inference_po 20G 1 7G 5 1G 1G 3 16 15.15 0.04 batch-jobs churn_feature_set_3_daily preventive_churn_ml_campaign_po 2 1 5G 5 1G 1G 5 26 24.82 0.03\\n\\nPurpose and Scope of this Document\\n\\nThe purpose and scope of this document is to enlist the steps for the deployment of below tables into the JioOne Prod environment.'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='Purpose and Scope of this Document\\n\\nThe purpose and scope of this document is to enlist the steps for the deployment of below tables into the JioOne Prod environment.\\n\\nTable Name connectivity_home.preventive_churn_ml_outages_po connectivity_home.preventive_churn_ml_ont_daily_agg_po connectivity_home.preventive_churn_ml_ont_po connectivity_home.preventive_churn_ml_hsi_po connectivity_home.preventive_churn_ml_stb_po connectivity_home.preventive_churn_ml_alarms_po connectivity_home.preventive_churn_ml_sub6_acqu_o connectivity_home.preventive_churn_ml_fiber_acqu_o connectivity_home.preventive_churn_ml_ubr_acqu_o connectivity_home.preventive_churn_ml_workorder_po connectivity_home.preventive_churn_ml_care_po connectivity_home.preventive_churn_ml_paymentsfeature_o connectivity_home.preventive_churn_ml_moat_po connectivity_home.preventive_churn_ml_model_inference_po connectivity_home.preventive_churn_ml_campaign_po\\n\\nDelivery Checklist\\n\\nDetails of Components used\\n\\nPlease lists all the components used in this release\\n\\nDAG files\\n\\nShell scripts\\n\\nPySpark scripts\\n\\nProperties file\\n\\nhql files\\n\\nChecklist\\n\\nDAG Name Description Size Attachment churn_feature_set_1_daily.txt DAG to create the  features of ont_daily,ont,alarms 2KB churn_feature_set_2_daily.txt DAG to create the  features of aqusition,hsi,stb,care,outages,workorder 4KB churn_feature_set_3_daily.txt DAG to create the  features of payments,moat,inference,campaign 2KB churn_features_purge_daily.txt DAG to purge the feature tables historic data 1KB churn_payments_purge_daily.txt DAG to purge the historic data of payments 1KB create_all_churn_tables_once.txxt DAG to create all the tables of preventive_churn_ml 1KB'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='File Name Description Size Attachment CI/CD Path ont_daily_aggregation.py Script to generate ont daily features 3.4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ ont_features_generation_new.py Script to generate ONT features 6KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ alarms_features_generation.py Script to generate alarms features 9KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ sub6_acquisition_features.py Script to generate sub6 acquisition features 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ fiber_acquisition_features.py Script to generate fiber acquisition features 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ ubr_acquisition_features.py Script to generate ubr acquisition features 5KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ hsi_features_generation.py Script to generate hsi features 11KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ stb_features_join.py Script to generate stb features 21KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ care_features_generation.py Script to generate care features 10KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ outages_features_generation.py Script to generate outages features 29KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ workorder_features_generation.py Script to generate workorder features 16KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ payments_feature_generation.py Script to generate payments features 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ moat_creation_script.py Script to generate moat features 23KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ Inference.py Script to generate inference features 15KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ campaign_generation_script.py Script to generate campaign features 23KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ spark_submit_alarms.sh Spark submit to trigger the alarms job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_campaign.sh Spark submit to trigger the campaign job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_care.sh Spark submit to trigger the care job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_fiber_acqu.sh Spark submit to trigger the fiber acquisition job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_hsi.sh Spark submit to trigger the hsi  job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_inference.sh Spark submit to trigger the inference job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_moat.sh Spark submit to trigger the moat job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_ont_daily.sh Spark submit to trigger the ont daily job 2KB'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_ont_daily.sh Spark submit to trigger the ont daily job 2KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_ont.sh Spark submit to trigger the ont job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_outages.sh Spark submit to trigger the outages job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_payments.sh Spark submit to trigger the payments job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_stb.sh Spark submit to trigger the stb job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_sub6_acqu.sh Spark submit to trigger the sub6 acquisition job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_ubr_acqu.sh Spark submit to trigger the ubr acquisition job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_workorder.sh Spark submit to trigger the workorder job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ create_all_churn_tables.sh Spark submit to trigger the create tables job 1KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ purge_churn_tables.sh Script to purge the historic data of churn tables 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ payments_hdfs_purging_script.sh Script to purge the historic data of payments data 1KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ deployment_details.properties Deployment properties file 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ create_all_churn_tables.hql Create table statement for churn tables 60KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/hive/'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='Data Paths\\n\\nSource Path\\n\\nPath /warehouse/tablespace/external/hive/connectivity_home.db/customer_complaints/home_customer_care_raw_pob/po /warehouse/tablespace/external/hive/devices.db/data_collector/ont/raw/po /warehouse/tablespace/external/hive/devices.db/sqm_alarm_kpi/fttx_sqm_hpsm_duration_kpi_daily_po /warehouse/tablespace/external/hive/devices.db/sqm_alarm_kpi/fttx_sqm_ams_duration_kpi_daily_po /warehouse/tablespace/external/hive/customer.db/cust360/airfiber_cust_crm_360_o /warehouse/tablespace/external/hive/customer.db/cust360/fttx_cust_crm_360_o /warehouse/tablespace/external/hive/customer.db/cust360/ubr_cust_crm_360_o /warehouse/tablespace/external/hive/connectivity_home.db/hsi/home_product_hsi_aggregate/po /warehouse/tablespace/external/hive/connectivity_home.db/home_stb_ott/home_stb_ott_app_usage_kpi_po /warehouse/tablespace/external/hive/connectivity_home.db/home_stb/home_stb_app_crash /warehouse/tablespace/external/hive/connectivity_home.db/home_stb_ott/home_ott_app_consumption_kpi_po /warehouse/tablespace/external/hive/connectivity_home.db/homeconnect/kpi/home/workorder_aggregate_kpi /data/mercury/fiber_churn_2024_paymentsfeature/ /warehouse/tablespace/external/hive/customer.db/crm_fixedline/crm_fixedline_unique_customers_o /warehouse/tablespace/external/hive/customer.db/crm/airfiber_crm_unique_customers_o /warehouse/tablespace/external/hive/customer.db/cust360/fttx_cust_crm_360_o /warehouse/tablespace/external/hive/customer.db/cust360/airfiber_cust_crm_360_o /warehouse/tablespace/external/hive/customer.db/cust360/ubr_cust_crm_360_o\\n\\nDestination Path\\n\\nTable Name Path connectivity_home.preventive_churn_ml_outages_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_outages connectivity_home.preventive_churn_ml_ont_daily_agg_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_ont_daily_agg connectivity_home.preventive_churn_ml_ont_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_ont connectivity_home.preventive_churn_ml_hsi_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_hsi connectivity_home.preventive_churn_ml_stb_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_stb connectivity_home.preventive_churn_ml_alarms_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_exal connectivity_home.preventive_churn_ml_sub6_acqu_o /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_airfiber_acqu connectivity_home.preventive_churn_ml_fiber_acqu_o /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_fiber_acqu connectivity_home.preventive_churn_ml_ubr_acqu_o /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_ubr_acqu connectivity_home.preventive_churn_ml_workorder_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_workorder connectivity_home.preventive_churn_ml_care_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_care connectivity_home.preventive_churn_ml_paymentsfeature_o /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_paymentsfeature connectivity_home.preventive_churn_ml_moat_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_moat connectivity_home.preventive_churn_ml_model_inference_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_model_inference connectivity_home.preventive_churn_ml_campaign_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_campaign\\n\\nSource Information'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='Source Information\\n\\nSource Owner Name Source Owner Approval (TOPS) Jio.TopsJBDL@ril.com Hive tables already present in JioOne Prod environment\\n\\nTable Name\\n\\nSource Table Name\\n\\nTable Name connectivity_home.home_customer_care_raw_pob devices.dc_ont_raw_po devices.fttx_sqm_hpsm_duration_kpi_daily_po devices.fttx_sqm_ams_duration_kpi_daily_po customer.airfiber_cust_crm_360_o customer.fttx_cust_crm_360_o customer.ubr_cust_crm_360_o connectivity_home.home_product_hsi_aggregate_po connectivity_home.home_stb_ott_app_usage_kpi_po connectivity_home.home_stb_app_crash_anr_po connectivity_home.home_ott_app_consumption_kpi_po connectivity_home.hc_workorder_aggregate_kpi_o customer.home_cust_stb_view customer.crm_fixedline_unique_customers_o customer.airfiber_crm_unique_customers_o customer.home_cust_crm_360_view customer.fttx_cust_crm_360_o customer.aifiber_cust_crm_360_o customer.ubr_cust_crm_360_o\\n\\n\\n\\nFinal/Intermediate Table Name & Description\\n\\nTable Name Table Type (External/Manage) Data Frequency (Daily once, streaming etc) Partition (If any) Table Description connectivity_home.preventive_churn_ml_outages_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains all the features related to outages connectivity_home.preventive_churn_ml_ont_daily_agg_po External Daily partition_date This table contains the sum aggregation of CPU, mem, rssi columns of ONT device connectivity_home.preventive_churn_ml_ont_po External Once in 5 days (6 bill dates in a month) target_fed This table contains ont usage performance features for each customer connectivity_home.preventive_churn_ml_hsi_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains hsi usage features related to usage in mb, no. of users connected, etc. connectivity_home.preventive_churn_ml_stb_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains the all STB related features like engagement, experience of all the customers connectivity_home.preventive_churn_ml_alarms_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains number of alarms and alarms duration features of each customer connectivity_home.preventive_churn_ml_sub6_acqu_o External Once in 5 days (6 bill dates in a month) N/A This table AirFiber Sub6 customers acquisition features such as location and device related features connectivity_home.preventive_churn_ml_fiber_acqu_o External Once in 5 days (6 bill dates in a month) N/A This table Fiber customers acquisition features such as location and device related features connectivity_home.preventive_churn_ml_ubr_acqu_o External Once in 5 days (6 bill dates in a month) N/A This table AirFiber Ubr customers acquisition features such as location and device related features connectivity_home.preventive_churn_ml_workorder_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains frequency and type of  workorders raised by each customer. connectivity_home.preventive_churn_ml_care_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains features related customer care connectivity_home.preventive_churn_ml_paymentsfeature_o External Once in 5 days (6 bill dates in a month) N/A This table contains billdate-wise features of payments data of customers connectivity_home.preventive_churn_ml_moat_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains all perspective features connectivity_home.preventive_churn_ml_model_inference_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains predicted output from the ML model for all customer IDs connectivity_home.preventive_churn_ml_campaign_po External Once in 5 days (6 bill dates in a month) partition_date This table contains customer IDs and their details for targeted telecalling and WhatsApp campaigns based on churn probability'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='Tables/Schema Created in Development Yes Atlas Tag Applied for PII/SPI Column Yes Verified By Siddharth2.S\\n\\nKAFKA Topic Details:\\n\\nNot Applicable\\n\\nES Index Details:\\n\\nNot Applicable\\n\\nData Dictionary of all table created\\n\\nFinal/Intermediate Table Sample\\n\\nsample records of all the table created/Altered\\n\\nRetention Period\\n\\nTable Name Retention connectivity_home.preventive_churn_ml_outages_po 15 days connectivity_home.preventive_churn_ml_ont_daily_agg_po 20 days connectivity_home.preventive_churn_ml_ont_po 15 days connectivity_home.preventive_churn_ml_hsi_po 15 days connectivity_home.preventive_churn_ml_stb_po 15 days connectivity_home.preventive_churn_ml_alarms_po 15 days connectivity_home.preventive_churn_ml_sub6_acqu_o - connectivity_home.preventive_churn_ml_fiber_acqu_o - connectivity_home.preventive_churn_ml_ubr_acqu_o - connectivity_home.preventive_churn_ml_workorder_po 15 days connectivity_home.preventive_churn_ml_care_po 15 days connectivity_home.preventive_churn_ml_paymentsfeature_o - connectivity_home.preventive_churn_ml_moat_po 1 year 3 months connectivity_home.preventive_churn_ml_model_inference_po 180 days connectivity_home.preventive_churn_ml_campaign_po 180 days\\n\\nARB Detail\\n\\nARB No ARB Date ARB Attendees ARB Approved by Approval Mail #20250022 04/04/2025 Rikhav.Mamania, Jagjitsingh.Uppal, Saurav.Suman Rikhav.Mamania, Saurav.Suman\\n\\nDFD/Algorithm/Sudo Logic Implemented\\n\\nJobs mapping\\n\\nReference Data\\n\\nBusiness End-users\\n\\nProvide Business User Information and Business Approvals\\n\\nShantanu.Mukherjee, Sanjive.Kumar\\n\\nDashboards & Reports currently running\\n\\nReconciliation Control logs\\n\\nImpact Analysis\\n\\nThis is a new release and will not impact any existing pipelines in JioOne Prod.\\n\\nBusiness/Systems Impact\\n\\nNot Applicable\\n\\nPre-Execution Steps\\n\\nService Account : devices_fttx\\n\\nAD Group – GALL—JBDL—FTTX—PREVCHURN\\n\\nCI location –\\n\\n/data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/deployment_details.properties\\n\\nPlease check for artifacts and files at the CI location.\\n\\nPlease approve the merge request for the JIOONE_CDP_PROD_AIRFLOW for DAG deployment.\\n\\nExecution Steps\\n\\nApplication Team Steps\\n\\nRun the rundeck for the below paths with respective properties\\n\\napp/deployable_artifacts/udeploy/coe-business-cdp-integration/scripts/init/home/preventive_churn_ml/deployment_details.properties\\n\\n(Artifacts & Jar, HQL & scripts, DAG deployment – ‘YES’)\\n\\nLogin using “devices_fttx” user.\\n\\nCheck for artifacts & files at the below CD location –\\n\\n/data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/\\n\\n/data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/hive/\\n\\n/data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submits/\\n\\nEnable the DAG - create_all_churn_tables_once\\n\\nEnable the remaining DAG’s -\\n\\nChurn_feature_set_1_daily\\n\\nChurn_feature_set_2_daily\\n\\nChurn_feature_set_3_daily\\n\\nChurn_features_purge_daily\\n\\nChurn_payments_purge_daily\\n\\nHadoop Admin Team Steps\\n\\nDatabase Team Steps\\n\\nMiddleware Team Steps\\n\\nReprocessing Steps in case of failure\\n\\nIn case of DAG failure, please rerun the DAGs.\\n\\nIn case of processing errors, please update the scripts as per error and merge the changes into the master branch.\\n\\nRestart the process from rundeck step.\\n\\nAD Group / User Info\\n\\nGALL group name Data source name Business group name GALL—JBDL—FTTX—PREVCHURN Preventive Churn ML FTTX Insights\\n\\nTableau Access (Ranger Policy)\\n\\nTableau Service Id\\t: Not Aplicable\\n\\nHive Table Names\\t: Not Aplicable\\n\\nHive Table Path\\t: Not Aplicable\\n\\nSeparately Privacy/Infosec Approval Mail Attachment:\\tJio.PrivacyOps@ril.com\\n\\n(This is additional & separately required)\\n\\nSanity Steps\\n\\nCheck files in destination hdfs path\\n\\nQuery on the table\\n\\nCheck if data is getting inserted to hive table, run the query\\n\\nSelect * from connectivity_home.preventive_churn_ml_sub6_acqu_o  limit 5;'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='Check files in destination hdfs path\\n\\nQuery on the table\\n\\nCheck if data is getting inserted to hive table, run the query\\n\\nSelect * from connectivity_home.preventive_churn_ml_sub6_acqu_o  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_fiber_acqu_o  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_ubr_acqu_o  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_hsi_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_stb_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_care_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_outages_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_workorder_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_ont_daily_po limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_ont_po limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_alarms_po limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_paymentsfeature_o  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_moat_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_model_inference_po limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_campaign_po  limit 5;\\n\\nRoll Back Steps\\n\\nDrop the created partitions in all the destination tables.\\n\\nDisable all the DAGS with ‘preventive_churn_ml’ tag.\\n\\nDrop the following tables to roll back the deployment of the table.\\n\\nconnectivity_home.preventive_churn_ml_outages_po\\n\\nconnectivity_home.preventive_churn_ml_ont_daily_agg_po\\n\\nconnectivity_home.preventive_churn_ml_ont_po\\n\\nconnectivity_home.preventive_churn_ml_hsi_po\\n\\nconnectivity_home.preventive_churn_ml_stb_po\\n\\nconnectivity_home.preventive_churn_ml_alarms_po\\n\\nconnectivity_home.preventive_churn_ml_sub6_acqu_o\\n\\nconnectivity_home.preventive_churn_ml_fiber_acqu_o\\n\\nconnectivity_home.preventive_churn_ml_ubr_acqu_o\\n\\nconnectivity_home.preventive_churn_ml_workorder_po\\n\\nconnectivity_home.preventive_churn_ml_care_po\\n\\nconnectivity_home.preventive_churn_ml_paymentsfeature_o\\n\\nconnectivity_home.preventive_churn_ml_moat_po\\n\\nconnectivity_home.preventive_churn_ml_model_inference_po\\n\\nconnectivity_home.preventive_churn_ml_campaign_po\\n\\nData Migration\\n\\nInfosec Approvals\\n\\nPII Tagging Detail\\n\\n\\n\\nRanger Policies\\n\\nPlease ensure to mention the service ID should have access to the schema and PII fields (If any) and mention the source tables and PII fields. Add permission/group in ranger.\\n\\nService ID Hive Schema Name Destination Hive Table Name Destination HDFS Path PII Field of Source Table\\n\\nTest Result\\n\\nDev Test Stats\\n\\nProvide development test stats \\n\\nTable Name (To be created) Avg. data size per day Avg. Record Count Per Day Avg. No of Files Per Day Avg. Size Per File Connectivity_home.preventive_churn_ml_sub6_acqu_o 0.07 GB 4,713,132 (4.7M) 1 0.07 GB Connectivity_home.preventive_churn_ml_fiber_acqu_o 0.12 GB 9,530,933 (9.5M) 1 0.12 GB Connectivity_home.preventive_churn_ml_ubr_acqu_o 0.02 GB 115,307 (115k) 1 0.02 GB Connectivity_home.preventive_churn_ml_hsi_po 1.25 GB 23,762,709 (23.7M) 10 0.125 GB Connectivity_home.preventive_churn_ml_stb_po 0.54 GB 9,519,419 (9.5M) 4 0.135 GB Connectivity_home.preventive_churn_ml_outages_po 0.11 GB 1,120,436 (1.1M) 1 0.11 GB Connectivity_home.preventive_churn_ml_care_po 0.42 GB 7,733,819 (7.7M) 4 0.105 GB Connectivity_home.preventive_churn_ml_workorder_po 0.42 GB 4156950 (4.1M) 4 0.105 GB Connectivity_home.preventive_churn_ml_ont_agg_daily_po 0.76 GB 5,376,866(5.3 M) 4 0.19 GB Connectivity_home.preventive_churn_ml_ont_po 0.61 GB 6,147,215 (6.1M) 4 0.152 GB Connectivity_home.preventive_churn_ml_alarms_po 0.43 GB 9,325,780 (9.3M) 4 0.107 GB Connectivity_home.preventive_churn_ml_paymentsfeature_o 0.11 GB 1,575,085 (1.5M) 1 0.11 GB Connectivity_home.preventive_churn_ml_moat_po 0.35 GB 1,202,713 (1.2M) 4 0.087 GB Connectivity_home.preventive_churn_ml_model_inference_po 0.04 GB 957,006 (1M) 1 0.04 GB Connectivity_home.preventive_churn_ml_campaign_po 0.01 GB 318,141 (300k) 1 0.01 GB'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='Elastic Name Elastic Index (No of Shards) Elastic Index Size (Primary Shards) Elastic Index Purging (ILM Policy)\\n\\nDev Test Screenshot\\n\\nThe above zip file contains the all the yarn and graphana screenshots of scripts\\n\\nDr Elephant/Spark lint/WXM Screenshot\\n\\nAirflow Screenshot\\n\\nPage 17 of 19'),\n",
       " Document(metadata={'source': './rag_data/Release_2649.docx'}, page_content='Date of version: 14.05.2025 Created by: Marrapu.Ramlokesh, Siddharth2.S Confidentiality level: Internal use\\n\\nChange history\\n\\nDate Version Created by Description of change DD.MM.YYYY\\n\\nIntroduction\\n\\nBusiness/Use Case Requirement Brief\\n\\nTo identify AirFiber and Fiber\\xa0customers at risk of churning and enriching the output with additional actionable insights (indicators of problems each customer might have faced). The aim is to enable proactive measures to retain customers and reduce churn.\\n\\nBusiness Description of Change\\n\\nWe are developing a model to predict customer churn probability using multiple data points, including payment history, complaints, outages, and network issues. Customers will be sorted on every billing date based on their likelihood of churn (i.e., probability of churning). This segmentation will allow stakeholders to prioritize interventions and retention strategies.\\n\\nBusiness Usecase\\n\\nJioOne||Campaigns||Retention and Termination||Preventive Churn ML||KPIs\\n\\nBusiness Benefits\\n\\nEnhanced Customer Retention: Proactively identifying at-risk customers and resolving their issues will improve satisfaction and loyalty, reducing overall churn.\\n\\nImproved Revenue: By focusing on customers at high risk of churn and retaining them, the business safeguards recurring revenue streams.\\n\\nData-Driven Decisions: Providing actionable insights into customer health allows for targeted campaigns and better service prioritization.\\n\\nCustomer Experience Improvement: Prioritizing service restoration and addressing complaints for at-risk customers improves their overall experience and perception of Jio services.\\n\\nList of KPIs\\n\\nconnectivity_home.preventive_churn_ml_moat_po\\n\\nconnectivity_home.preventive_churn_ml_model_inference_po\\n\\nconnectivity_home.preventive_churn_ml_campaign_po\\n\\nQueue Analysis\\n\\nQueue Name DAG name Table name Driver_mem Driver cores Exec_mem Executor cores driver mem overhead exec mem overhead Max_executors Number of containers used time taken in min %queue_used batch-jobs churn_feature_set_2_daily preventive_churn_ml_sub6_acqu_o 1G 1 3G 3 1G 1G 3 10 5.21 0.02 batch-jobs churn_feature_set_2_daily preventive_churn_ml_fiber_acqu_o 1G 1 3G 3 1G 1G 3 10 4.80 0.02 batch-jobs churn_feature_set_2_daily preventive_churn_ml_ubr_acqu_o 1G 1 3G 3 1G 1G 3 10 3.44 0.01 batch-jobs churn_feature_set_2_daily preventive_churn_ml_hsi_po 2G 1 8G 5 1G 1G 10 51 44.52 0.08 batch-jobs churn_feature_set_2_daily preventive_churn_ml_stb_po 3G 1 22G 5 1G 2G 22 111 29.5 0.47 batch-jobs churn_feature_set_2_daily preventive_churn_ml_care_po 2G 1 8G 5 1G 2G 10 51 42.78 0.08 batch-jobs churn_feature_set_2_daily preventive_churn_ml_outages_po 2G 1 8G 5 1G 1G 10 51 14.82 0.07 batch-jobs churn_feature_set_2_daily preventive_churn_ml_workorder_po 2G 1 8G 5 1G 1G 10 51 10.25 0.06 batch-jobs churn_feature_set_3_daily preventive_churn_ml_paymentsfeature_o 1G 1 1G 3 1G 1G 1 4 1.43 0.00 batch-jobs churn_feature_set_1_daily preventive_churn_ml_ont_daily_agg_po 2G 1 8G 5 1G 1G 10 51 15.40 0.08 batch-jobs churn_feature_set_1_daily preventive_churn_ml_ont_po 1G 1 3G 5 1G 1G 5 26 9.10 0.05 batch-jobs churn_feature_set_1_daily preventive_churn_ml_alarms_po 2G 1 10G 5 1G 2G 24 121 88.00 0.24 batch-jobs churn_feature_set_3_daily preventive_churn_ml_moat_po 2G 1 5G 5 1G 1G 10 51 5.63 0.06 batch-jobs churn_feature_set_3_daily preventive_churn_ml_inference_po 20G 1 7G 5 1G 1G 3 16 15.15 0.04 batch-jobs churn_feature_set_3_daily preventive_churn_ml_campaign_po 2 1 5G 5 1G 1G 5 26 24.82 0.03\\n\\nPurpose and Scope of this Document\\n\\nThe purpose and scope of this document is to enlist the steps for the deployment of below tables into the JioOne Prod environment.\\n\\nTable Name connectivity_home.preventive_churn_ml_outages_po connectivity_home.preventive_churn_ml_ont_daily_agg_po connectivity_home.preventive_churn_ml_ont_po connectivity_home.preventive_churn_ml_hsi_po connectivity_home.preventive_churn_ml_stb_po connectivity_home.preventive_churn_ml_alarms_po connectivity_home.preventive_churn_ml_sub6_acqu_o connectivity_home.preventive_churn_ml_fiber_acqu_o connectivity_home.preventive_churn_ml_ubr_acqu_o connectivity_home.preventive_churn_ml_workorder_po connectivity_home.preventive_churn_ml_care_po connectivity_home.preventive_churn_ml_paymentsfeature_o connectivity_home.preventive_churn_ml_moat_po connectivity_home.preventive_churn_ml_model_inference_po connectivity_home.preventive_churn_ml_campaign_po\\n\\nDelivery Checklist\\n\\nDetails of Components used\\n\\nPlease lists all the components used in this release\\n\\nDAG files\\n\\nShell scripts\\n\\nPySpark scripts\\n\\nProperties file\\n\\nhql files\\n\\nChecklist\\n\\nDAG Name Description Size Attachment churn_feature_set_1_daily.txt DAG to create the  features of ont_daily,ont,alarms 2KB churn_feature_set_2_daily.txt DAG to create the  features of aqusition,hsi,stb,care,outages,workorder 4KB churn_feature_set_3_daily.txt DAG to create the  features of payments,moat,inference,campaign 2KB churn_features_purge_daily.txt DAG to purge the feature tables historic data 1KB churn_payments_purge_daily.txt DAG to purge the historic data of payments 1KB create_all_churn_tables_once.txxt DAG to create all the tables of preventive_churn_ml 1KB\\n\\nFile Name Description Size Attachment CI/CD Path ont_daily_aggregation.py Script to generate ont daily features 3.4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ ont_features_generation_new.py Script to generate ONT features 6KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ alarms_features_generation.py Script to generate alarms features 9KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ sub6_acquisition_features.py Script to generate sub6 acquisition features 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ fiber_acquisition_features.py Script to generate fiber acquisition features 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ ubr_acquisition_features.py Script to generate ubr acquisition features 5KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ hsi_features_generation.py Script to generate hsi features 11KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ stb_features_join.py Script to generate stb features 21KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ care_features_generation.py Script to generate care features 10KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ outages_features_generation.py Script to generate outages features 29KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ workorder_features_generation.py Script to generate workorder features 16KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ payments_feature_generation.py Script to generate payments features 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ moat_creation_script.py Script to generate moat features 23KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ Inference.py Script to generate inference features 15KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ campaign_generation_script.py Script to generate campaign features 23KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ spark_submit_alarms.sh Spark submit to trigger the alarms job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_campaign.sh Spark submit to trigger the campaign job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_care.sh Spark submit to trigger the care job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_fiber_acqu.sh Spark submit to trigger the fiber acquisition job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_hsi.sh Spark submit to trigger the hsi  job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_inference.sh Spark submit to trigger the inference job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_moat.sh Spark submit to trigger the moat job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_ont_daily.sh Spark submit to trigger the ont daily job 2KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_ont.sh Spark submit to trigger the ont job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_outages.sh Spark submit to trigger the outages job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_payments.sh Spark submit to trigger the payments job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_stb.sh Spark submit to trigger the stb job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_sub6_acqu.sh Spark submit to trigger the sub6 acquisition job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_ubr_acqu.sh Spark submit to trigger the ubr acquisition job 4KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ spark_submit_workorder.sh Spark submit to trigger the workorder job 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submit_scripts/ create_all_churn_tables.sh Spark submit to trigger the create tables job 1KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ purge_churn_tables.sh Script to purge the historic data of churn tables 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ payments_hdfs_purging_script.sh Script to purge the historic data of payments data 1KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ deployment_details.properties Deployment properties file 3KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/ create_all_churn_tables.hql Create table statement for churn tables 60KB /data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/hive/\\n\\nData Paths\\n\\nSource Path\\n\\nPath /warehouse/tablespace/external/hive/connectivity_home.db/customer_complaints/home_customer_care_raw_pob/po /warehouse/tablespace/external/hive/devices.db/data_collector/ont/raw/po /warehouse/tablespace/external/hive/devices.db/sqm_alarm_kpi/fttx_sqm_hpsm_duration_kpi_daily_po /warehouse/tablespace/external/hive/devices.db/sqm_alarm_kpi/fttx_sqm_ams_duration_kpi_daily_po /warehouse/tablespace/external/hive/customer.db/cust360/airfiber_cust_crm_360_o /warehouse/tablespace/external/hive/customer.db/cust360/fttx_cust_crm_360_o /warehouse/tablespace/external/hive/customer.db/cust360/ubr_cust_crm_360_o /warehouse/tablespace/external/hive/connectivity_home.db/hsi/home_product_hsi_aggregate/po /warehouse/tablespace/external/hive/connectivity_home.db/home_stb_ott/home_stb_ott_app_usage_kpi_po /warehouse/tablespace/external/hive/connectivity_home.db/home_stb/home_stb_app_crash /warehouse/tablespace/external/hive/connectivity_home.db/home_stb_ott/home_ott_app_consumption_kpi_po /warehouse/tablespace/external/hive/connectivity_home.db/homeconnect/kpi/home/workorder_aggregate_kpi /data/mercury/fiber_churn_2024_paymentsfeature/ /warehouse/tablespace/external/hive/customer.db/crm_fixedline/crm_fixedline_unique_customers_o /warehouse/tablespace/external/hive/customer.db/crm/airfiber_crm_unique_customers_o /warehouse/tablespace/external/hive/customer.db/cust360/fttx_cust_crm_360_o /warehouse/tablespace/external/hive/customer.db/cust360/airfiber_cust_crm_360_o /warehouse/tablespace/external/hive/customer.db/cust360/ubr_cust_crm_360_o\\n\\nDestination Path\\n\\nTable Name Path connectivity_home.preventive_churn_ml_outages_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_outages connectivity_home.preventive_churn_ml_ont_daily_agg_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_ont_daily_agg connectivity_home.preventive_churn_ml_ont_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_ont connectivity_home.preventive_churn_ml_hsi_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_hsi connectivity_home.preventive_churn_ml_stb_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_stb connectivity_home.preventive_churn_ml_alarms_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_exal connectivity_home.preventive_churn_ml_sub6_acqu_o /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_airfiber_acqu connectivity_home.preventive_churn_ml_fiber_acqu_o /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_fiber_acqu connectivity_home.preventive_churn_ml_ubr_acqu_o /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_ubr_acqu connectivity_home.preventive_churn_ml_workorder_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_workorder connectivity_home.preventive_churn_ml_care_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_care connectivity_home.preventive_churn_ml_paymentsfeature_o /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_paymentsfeature connectivity_home.preventive_churn_ml_moat_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_moat connectivity_home.preventive_churn_ml_model_inference_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_model_inference connectivity_home.preventive_churn_ml_campaign_po /warehouse/tablespace/external/hive/connectivity_home.db/preventive_churn_ml/preventive_churn_ml_campaign\\n\\nSource Information\\n\\nSource Owner Name Source Owner Approval (TOPS) Jio.TopsJBDL@ril.com Hive tables already present in JioOne Prod environment\\n\\nTable Name\\n\\nSource Table Name\\n\\nTable Name connectivity_home.home_customer_care_raw_pob devices.dc_ont_raw_po devices.fttx_sqm_hpsm_duration_kpi_daily_po devices.fttx_sqm_ams_duration_kpi_daily_po customer.airfiber_cust_crm_360_o customer.fttx_cust_crm_360_o customer.ubr_cust_crm_360_o connectivity_home.home_product_hsi_aggregate_po connectivity_home.home_stb_ott_app_usage_kpi_po connectivity_home.home_stb_app_crash_anr_po connectivity_home.home_ott_app_consumption_kpi_po connectivity_home.hc_workorder_aggregate_kpi_o customer.home_cust_stb_view customer.crm_fixedline_unique_customers_o customer.airfiber_crm_unique_customers_o customer.home_cust_crm_360_view customer.fttx_cust_crm_360_o customer.aifiber_cust_crm_360_o customer.ubr_cust_crm_360_o\\n\\n\\n\\nFinal/Intermediate Table Name & Description\\n\\nTable Name Table Type (External/Manage) Data Frequency (Daily once, streaming etc) Partition (If any) Table Description connectivity_home.preventive_churn_ml_outages_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains all the features related to outages connectivity_home.preventive_churn_ml_ont_daily_agg_po External Daily partition_date This table contains the sum aggregation of CPU, mem, rssi columns of ONT device connectivity_home.preventive_churn_ml_ont_po External Once in 5 days (6 bill dates in a month) target_fed This table contains ont usage performance features for each customer connectivity_home.preventive_churn_ml_hsi_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains hsi usage features related to usage in mb, no. of users connected, etc. connectivity_home.preventive_churn_ml_stb_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains the all STB related features like engagement, experience of all the customers connectivity_home.preventive_churn_ml_alarms_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains number of alarms and alarms duration features of each customer connectivity_home.preventive_churn_ml_sub6_acqu_o External Once in 5 days (6 bill dates in a month) N/A This table AirFiber Sub6 customers acquisition features such as location and device related features connectivity_home.preventive_churn_ml_fiber_acqu_o External Once in 5 days (6 bill dates in a month) N/A This table Fiber customers acquisition features such as location and device related features connectivity_home.preventive_churn_ml_ubr_acqu_o External Once in 5 days (6 bill dates in a month) N/A This table AirFiber Ubr customers acquisition features such as location and device related features connectivity_home.preventive_churn_ml_workorder_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains frequency and type of  workorders raised by each customer. connectivity_home.preventive_churn_ml_care_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains features related customer care connectivity_home.preventive_churn_ml_paymentsfeature_o External Once in 5 days (6 bill dates in a month) N/A This table contains billdate-wise features of payments data of customers connectivity_home.preventive_churn_ml_moat_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains all perspective features connectivity_home.preventive_churn_ml_model_inference_po External Once in 5 days (6 bill dates in a month) feature_end_date This table contains predicted output from the ML model for all customer IDs connectivity_home.preventive_churn_ml_campaign_po External Once in 5 days (6 bill dates in a month) partition_date This table contains customer IDs and their details for targeted telecalling and WhatsApp campaigns based on churn probability\\n\\nTables/Schema Created in Development Yes Atlas Tag Applied for PII/SPI Column Yes Verified By Siddharth2.S\\n\\nKAFKA Topic Details:\\n\\nNot Applicable\\n\\nES Index Details:\\n\\nNot Applicable\\n\\nData Dictionary of all table created\\n\\nFinal/Intermediate Table Sample\\n\\nsample records of all the table created/Altered\\n\\nRetention Period\\n\\nTable Name Retention connectivity_home.preventive_churn_ml_outages_po 15 days connectivity_home.preventive_churn_ml_ont_daily_agg_po 20 days connectivity_home.preventive_churn_ml_ont_po 15 days connectivity_home.preventive_churn_ml_hsi_po 15 days connectivity_home.preventive_churn_ml_stb_po 15 days connectivity_home.preventive_churn_ml_alarms_po 15 days connectivity_home.preventive_churn_ml_sub6_acqu_o - connectivity_home.preventive_churn_ml_fiber_acqu_o - connectivity_home.preventive_churn_ml_ubr_acqu_o - connectivity_home.preventive_churn_ml_workorder_po 15 days connectivity_home.preventive_churn_ml_care_po 15 days connectivity_home.preventive_churn_ml_paymentsfeature_o - connectivity_home.preventive_churn_ml_moat_po 1 year 3 months connectivity_home.preventive_churn_ml_model_inference_po 180 days connectivity_home.preventive_churn_ml_campaign_po 180 days\\n\\nARB Detail\\n\\nARB No ARB Date ARB Attendees ARB Approved by Approval Mail #20250022 04/04/2025 Rikhav.Mamania, Jagjitsingh.Uppal, Saurav.Suman Rikhav.Mamania, Saurav.Suman\\n\\nDFD/Algorithm/Sudo Logic Implemented\\n\\nJobs mapping\\n\\nReference Data\\n\\nBusiness End-users\\n\\nProvide Business User Information and Business Approvals\\n\\nShantanu.Mukherjee, Sanjive.Kumar\\n\\nDashboards & Reports currently running\\n\\nReconciliation Control logs\\n\\nImpact Analysis\\n\\nThis is a new release and will not impact any existing pipelines in JioOne Prod.\\n\\nBusiness/Systems Impact\\n\\nNot Applicable\\n\\nPre-Execution Steps\\n\\nService Account : devices_fttx\\n\\nAD Group – GALL—JBDL—FTTX—PREVCHURN\\n\\nCI location –\\n\\n/data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/deployment_details.properties\\n\\nPlease check for artifacts and files at the CI location.\\n\\nPlease approve the merge request for the JIOONE_CDP_PROD_AIRFLOW for DAG deployment.\\n\\nExecution Steps\\n\\nApplication Team Steps\\n\\nRun the rundeck for the below paths with respective properties\\n\\napp/deployable_artifacts/udeploy/coe-business-cdp-integration/scripts/init/home/preventive_churn_ml/deployment_details.properties\\n\\n(Artifacts & Jar, HQL & scripts, DAG deployment – ‘YES’)\\n\\nLogin using “devices_fttx” user.\\n\\nCheck for artifacts & files at the below CD location –\\n\\n/data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/\\n\\n/data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/hive/\\n\\n/data/appdata2/proddeployment/connectivity_home/home/preventive_churn_ml/preventive_churn_ml/init/spark_submits/\\n\\nEnable the DAG - create_all_churn_tables_once\\n\\nEnable the remaining DAG’s -\\n\\nChurn_feature_set_1_daily\\n\\nChurn_feature_set_2_daily\\n\\nChurn_feature_set_3_daily\\n\\nChurn_features_purge_daily\\n\\nChurn_payments_purge_daily\\n\\nHadoop Admin Team Steps\\n\\nDatabase Team Steps\\n\\nMiddleware Team Steps\\n\\nReprocessing Steps in case of failure\\n\\nIn case of DAG failure, please rerun the DAGs.\\n\\nIn case of processing errors, please update the scripts as per error and merge the changes into the master branch.\\n\\nRestart the process from rundeck step.\\n\\nAD Group / User Info\\n\\nGALL group name Data source name Business group name GALL—JBDL—FTTX—PREVCHURN Preventive Churn ML FTTX Insights\\n\\nTableau Access (Ranger Policy)\\n\\nTableau Service Id\\t: Not Aplicable\\n\\nHive Table Names\\t: Not Aplicable\\n\\nHive Table Path\\t: Not Aplicable\\n\\nSeparately Privacy/Infosec Approval Mail Attachment:\\tJio.PrivacyOps@ril.com\\n\\n(This is additional & separately required)\\n\\nSanity Steps\\n\\nCheck files in destination hdfs path\\n\\nQuery on the table\\n\\nCheck if data is getting inserted to hive table, run the query\\n\\nSelect * from connectivity_home.preventive_churn_ml_sub6_acqu_o  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_fiber_acqu_o  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_ubr_acqu_o  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_hsi_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_stb_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_care_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_outages_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_workorder_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_ont_daily_po limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_ont_po limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_alarms_po limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_paymentsfeature_o  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_moat_po  limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_model_inference_po limit 5;\\n\\nSelect * from connectivity_home.preventive_churn_ml_campaign_po  limit 5;\\n\\nRoll Back Steps\\n\\nDrop the created partitions in all the destination tables.\\n\\nDisable all the DAGS with ‘preventive_churn_ml’ tag.\\n\\nDrop the following tables to roll back the deployment of the table.\\n\\nconnectivity_home.preventive_churn_ml_outages_po\\n\\nconnectivity_home.preventive_churn_ml_ont_daily_agg_po\\n\\nconnectivity_home.preventive_churn_ml_ont_po\\n\\nconnectivity_home.preventive_churn_ml_hsi_po\\n\\nconnectivity_home.preventive_churn_ml_stb_po\\n\\nconnectivity_home.preventive_churn_ml_alarms_po\\n\\nconnectivity_home.preventive_churn_ml_sub6_acqu_o\\n\\nconnectivity_home.preventive_churn_ml_fiber_acqu_o\\n\\nconnectivity_home.preventive_churn_ml_ubr_acqu_o\\n\\nconnectivity_home.preventive_churn_ml_workorder_po\\n\\nconnectivity_home.preventive_churn_ml_care_po\\n\\nconnectivity_home.preventive_churn_ml_paymentsfeature_o\\n\\nconnectivity_home.preventive_churn_ml_moat_po\\n\\nconnectivity_home.preventive_churn_ml_model_inference_po\\n\\nconnectivity_home.preventive_churn_ml_campaign_po\\n\\nData Migration\\n\\nInfosec Approvals\\n\\nPII Tagging Detail\\n\\n\\n\\nRanger Policies\\n\\nPlease ensure to mention the service ID should have access to the schema and PII fields (If any) and mention the source tables and PII fields. Add permission/group in ranger.\\n\\nService ID Hive Schema Name Destination Hive Table Name Destination HDFS Path PII Field of Source Table\\n\\nTest Result\\n\\nDev Test Stats\\n\\nProvide development test stats \\n\\nTable Name (To be created) Avg. data size per day Avg. Record Count Per Day Avg. No of Files Per Day Avg. Size Per File Connectivity_home.preventive_churn_ml_sub6_acqu_o 0.07 GB 4,713,132 (4.7M) 1 0.07 GB Connectivity_home.preventive_churn_ml_fiber_acqu_o 0.12 GB 9,530,933 (9.5M) 1 0.12 GB Connectivity_home.preventive_churn_ml_ubr_acqu_o 0.02 GB 115,307 (115k) 1 0.02 GB Connectivity_home.preventive_churn_ml_hsi_po 1.25 GB 23,762,709 (23.7M) 10 0.125 GB Connectivity_home.preventive_churn_ml_stb_po 0.54 GB 9,519,419 (9.5M) 4 0.135 GB Connectivity_home.preventive_churn_ml_outages_po 0.11 GB 1,120,436 (1.1M) 1 0.11 GB Connectivity_home.preventive_churn_ml_care_po 0.42 GB 7,733,819 (7.7M) 4 0.105 GB Connectivity_home.preventive_churn_ml_workorder_po 0.42 GB 4156950 (4.1M) 4 0.105 GB Connectivity_home.preventive_churn_ml_ont_agg_daily_po 0.76 GB 5,376,866(5.3 M) 4 0.19 GB Connectivity_home.preventive_churn_ml_ont_po 0.61 GB 6,147,215 (6.1M) 4 0.152 GB Connectivity_home.preventive_churn_ml_alarms_po 0.43 GB 9,325,780 (9.3M) 4 0.107 GB Connectivity_home.preventive_churn_ml_paymentsfeature_o 0.11 GB 1,575,085 (1.5M) 1 0.11 GB Connectivity_home.preventive_churn_ml_moat_po 0.35 GB 1,202,713 (1.2M) 4 0.087 GB Connectivity_home.preventive_churn_ml_model_inference_po 0.04 GB 957,006 (1M) 1 0.04 GB Connectivity_home.preventive_churn_ml_campaign_po 0.01 GB 318,141 (300k) 1 0.01 GB\\n\\nElastic Name Elastic Index (No of Shards) Elastic Index Size (Primary Shards) Elastic Index Purging (ILM Policy)\\n\\nDev Test Screenshot\\n\\nThe above zip file contains the all the yarn and graphana screenshots of scripts\\n\\nDr Elephant/Spark lint/WXM Screenshot\\n\\nAirflow Screenshot\\n\\nPage 17 of 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-28T17:43:48+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-28T17:43:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': './rag_data/Marrapu_Ramlokesh_resume.pdf', 'total_pages': 1, 'page': 0, 'page_label': ''}, page_content='+91 8688224967\\nramlokeshmarrapu@gmail.com\\nMarrapu Ramlokesh\\nLinkedIn: ramlokesh\\nWORK EXPERIENCE\\nData Scientist Aug 2023 - present\\nJio Platforms Limited\\nPreventive Customer Churn Prediction: Forecasting and Mitigating Churn\\n•Engineered predictive features inPySparkby integrating diverse data sources—acquisition, engagement,\\ncomplaints, outages, and payments—using time-series analysis and behavioral segmentation\\n•Developed anXGBoost model, capturing40%of churn cases in thetop 5%of high-risk customers monthly from a\\nhighly imbalanced 1:50 churn-to-non-churn dataset\\n•ImplementedSHAPvalues andcorrelation analysisfor churn explainability, enabling targeted campaigns for\\nhigh-risk customers from7M+analyzed monthly.\\nReal-time CCTV-based PPE Detection: Enhancing Workplace Safety with AI\\n•Developed and Deployed aPyTorch-basedPPE detection system usingEfficientNetwith specialized heads for\\nhelmets, fire-resistant suits, and boots, achieving a0.92 F1 scorewhile leveragingClass Activation Mapsfor\\nperformance evaluation and visualization\\n•Applied diversedata augmentationtechniques such as texture changes, perspective transforms, and object color\\nmodifications, resulting in a23% increasein accuracy across various scenarios and camera angles\\nJio SIM Chatbot: Streamlining Customer Inquiries and Support\\n•Developed an interactive chatbot for managing Jio SIM-related inquiries,similarity detection, LangChain, and\\nstrategic prompting, boosting query resolution speed and customer satisfaction\\n•Attained a40% reductionin employee time and effort in responding to customer queries, leading to improved\\noperational efficiency and cost savings\\nResearch Intern May 2022 - June 2022\\nNITK, surathkal\\nImage Caption Generator\\n•Built an end-to-end image captioning pipeline usingTensorFlow(pre-trainedCNN+LSTM decoder) for the Flickr30k\\ndataset, applying sequence modelling,feature extractionand beam-search inference.\\n•Deployed aFlask-based web demo for image upload and auto-caption generation, demonstrating skills incomputer\\nvision,NLPand prototype deployment.\\nEDUCATION\\nB. Tech in Electronics and Communication Engineering, NIT surathkal July 2019 - May 2023\\nKEY COURSES UNDERTAKEN:\\n•Deep Learning for Natural Language Processing, Artificial Intelligence and Machine learning, Image Processing\\n•Calculus, Linear Algebra and Differential Equations\\nSKILLS\\nTools and LanguagesPython, Linux, PySpark, Git, Docker, SQL , Langchain, TensorFlow\\nQuantitative SkillsNumpy, Pandas, OpenCV, Matplotlib, MySQL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-10-28T17:43:48+00:00', 'author': '', 'keywords': '', 'moddate': '2025-10-28T17:43:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': './rag_data/Marrapu_Ramlokesh_resume.pdf', 'total_pages': 1, 'page': 0, 'page_label': ''}, page_content='+91 8688224967\\nramlokeshmarrapu@gmail.com\\nMarrapu Ramlokesh\\nLinkedIn: ramlokesh\\nWORK EXPERIENCE\\nData Scientist Aug 2023 - present\\nJio Platforms Limited\\nPreventive Customer Churn Prediction: Forecasting and Mitigating Churn\\n•Engineered predictive features inPySparkby integrating diverse data sources—acquisition, engagement,\\ncomplaints, outages, and payments—using time-series analysis and behavioral segmentation\\n•Developed anXGBoost model, capturing40%of churn cases in thetop 5%of high-risk customers monthly from a\\nhighly imbalanced 1:50 churn-to-non-churn dataset\\n•ImplementedSHAPvalues andcorrelation analysisfor churn explainability, enabling targeted campaigns for\\nhigh-risk customers from7M+analyzed monthly.\\nReal-time CCTV-based PPE Detection: Enhancing Workplace Safety with AI\\n•Developed and Deployed aPyTorch-basedPPE detection system usingEfficientNetwith specialized heads for\\nhelmets, fire-resistant suits, and boots, achieving a0.92 F1 scorewhile leveragingClass Activation Mapsfor\\nperformance evaluation and visualization\\n•Applied diversedata augmentationtechniques such as texture changes, perspective transforms, and object color\\nmodifications, resulting in a23% increasein accuracy across various scenarios and camera angles\\nJio SIM Chatbot: Streamlining Customer Inquiries and Support\\n•Developed an interactive chatbot for managing Jio SIM-related inquiries,similarity detection, LangChain, and\\nstrategic prompting, boosting query resolution speed and customer satisfaction\\n•Attained a40% reductionin employee time and effort in responding to customer queries, leading to improved\\noperational efficiency and cost savings\\nResearch Intern May 2022 - June 2022\\nNITK, surathkal\\nImage Caption Generator\\n•Built an end-to-end image captioning pipeline usingTensorFlow(pre-trainedCNN+LSTM decoder) for the Flickr30k\\ndataset, applying sequence modelling,feature extractionand beam-search inference.\\n•Deployed aFlask-based web demo for image upload and auto-caption generation, demonstrating skills incomputer\\nvision,NLPand prototype deployment.\\nEDUCATION\\nB. Tech in Electronics and Communication Engineering, NIT surathkal July 2019 - May 2023\\nKEY COURSES UNDERTAKEN:\\n•Deep Learning for Natural Language Processing, Artificial Intelligence and Machine learning, Image Processing\\n•Calculus, Linear Algebra and Differential Equations\\nSKILLS\\nTools and LanguagesPython, Linux, PySpark, Git, Docker, SQL , Langchain, TensorFlow\\nQuantitative SkillsNumpy, Pandas, OpenCV, Matplotlib, MySQL')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, csv_loader, json_loader, TextLoader, UnstructuredExcelLoader, UnstructuredWordDocumentLoader, UnstructuredEmailLoader\n",
    "import os\n",
    "\n",
    "def document_loader(directory_path: None) -> list[str]:\n",
    "    \"\"\"Load documents from a directory using appropriate loaders based on file type.\"\"\"\n",
    "    if directory_path is None:\n",
    "        raise ValueError(\"directory_path must be provided\")\n",
    "\n",
    "    loaders = {\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.csv': csv_loader.CSVLoader,\n",
    "        '.json': json_loader.JSONLoader,\n",
    "        '.txt': TextLoader,\n",
    "        '.xlsx': UnstructuredExcelLoader,\n",
    "        '.xls': UnstructuredExcelLoader,\n",
    "        '.docx': UnstructuredWordDocumentLoader,\n",
    "        '.eml': UnstructuredEmailLoader,\n",
    "        \n",
    "    }\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            loader_class = loaders.get(ext)\n",
    "            if loader_class:\n",
    "                print(f\"Loading file: {file_path} with loader: {loader_class.__name__}\")\n",
    "                loader = loader_class(file_path)\n",
    "                documents.extend(loader.load_and_split())\n",
    "                documents.extend(loader.load())\n",
    "            else:\n",
    "                print(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "loaded_documents = document_loader(\"./rag_data\")\n",
    "loaded_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f60d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6abd7541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Datasets available Utility Comments National Family Health Survey (NFHS): National Family Health Survey - 4 : District Yes National Family Health Survey-5 : District Yes National Family Health Survey - 4 & 5: State Yes National Sample Survey Organisation (NSSO): National Sample Survey (NSS) Round 75 -Social Consumption - Health Yes Could we access logitudinal data ? Medical expenditure for treatment per spell Yes Medical and Non-Medical Expenditure Per Hositalised ChildBirth Yes Average Expenditure for Treatment Yes Percentage distribution of persons by coverage of scheme of health expenditure support Yes Average medical expenditure and non-medical expenditure by gender Yes The similar datasets can be found for NSS Round 71 as well. Yes Would be useful to build the longitudinal profile for all the parameters above Rural Health Statistics (RHS): State/UT Wise Number of Sub-Centres, Primary Health Centres (PHC), Community Health Centres (CHC) & Health and Wellness Centres (HWC) Functioning in Rural & Urban Areas Yes Rural Health Statistics: Statewise Statistics Yes Health Human Resources Across the National Mental Health Survey (NMHS) States Yes Sample Registration System (SRS): Projected levels of Total Fertility Rates (TFR) for India and Major States Yes Infant Mortality Rates (IMR) in India Yes Total Fertility Rate (TFR) by Residence in India and Major States Yes Demographic Indicators: Infant Mortality Rate (IMR) and Under-5 mortality rate (q5) Yes Accidental Deaths & Suicides in India Yes Integrated Disease Surveillance Programme (IDSP): State/Disease-wise Number of Outbreaks Reported Under Integrated Disease Surveillance Project (IDSP) Yes Needed Longitudinal Data for at least 5 years Species-wise Incidence of Livestock Diseases in India Yes Needed Longitudinal Data for at least 5 years Datasets in National Health Profile Socio-Economic Caste Census (SECC): Socio-Economic Caste Census-Caste Profile of Households Yes Socio-Economic Caste Census-Education Profile of Households Yes Socio-Economic Caste Census - Main Source of Income of Households Yes Socio-Economic Caste Census-Asset Ownership details of Households Yes'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dbad4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"OPENAI_API_BASE\")\n",
    "\n",
    "# print(\"KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "# print(\"BASE:\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "\n",
    "class EmbeddingMaker:\n",
    "    \"\"\"A class to create embeddings for documents.\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            backend: str = \"sentence_transformers\",\n",
    "            model_name: str = \"all-MiniLM-L12-v2\", \n",
    "            chunk_size: int = 512, \n",
    "            chunk_overlap: int = 50,\n",
    "            batch_size: int = 16,\n",
    "            openai_key: Optional[str] = None,\n",
    "            openai_model: Optional[str] = None \n",
    "        ):\n",
    "        self.backend = backend\n",
    "        self.embed_model = model_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.batch_size = batch_size\n",
    "        self.openai_key = openai_key\n",
    "        self.openai_model = openai_model\n",
    "        if backend == \"sentence_transformers\":\n",
    "            self.embed_model = SentenceTransformer(model_name)\n",
    "    \n",
    "\n",
    "    def chunk_documents(self, documents: list[Any]) -> list[Any]:\n",
    "        \"\"\"Chunk documents into smaller pieces.\"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"Created {len(chunks)} chunks from {len(documents)} documents.\")\n",
    "        return chunks\n",
    "    \n",
    "    def get_embeddings(self, chunks: list[Any]) -> Any:\n",
    "        \"\"\"Get the embedding model based on the specified backend.\"\"\"\n",
    "        if self.backend == \"sentence_transformers\":\n",
    "            return self._get_sentence_transformers_embeddings(chunks)\n",
    "        elif self.backend == \"openai\":\n",
    "            return self._get_openai_embeddings(chunks)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported embedding backend: {self.backend}\")\n",
    "        \n",
    "    def _get_sentence_transformers_embeddings(self, chunks: list[Any]) -> np.ndarray:\n",
    "        \"\"\"Get embeddings using Sentence Transformers.\"\"\"\n",
    "        texts= [chunk.page_content for chunk in chunks]\n",
    "        print(f\"Generating embeddings for {len(texts)} chunks using Sentence Transformers.\")\n",
    "        embeddings = self.embed_model.encode(texts, show_progress_bar=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def _get_openai_embeddings(self, chunks: list[Any]) -> np.ndarray:\n",
    "        \"\"\"Get embeddings using OpenAI.\"\"\"\n",
    "        embedder = OpenAIEmbeddings(\n",
    "            model=self.openai_model\n",
    "        )\n",
    "        texts= [chunk.page_content for chunk in chunks]\n",
    "        print(f\"Generating embeddings for {len(texts)} chunks using OpenAI.\")\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i : i + self.batch_size]\n",
    "            embeddings = embedder.embed_documents(batch)\n",
    "            all_embeddings.append(embeddings) \n",
    "        return np.vstack(all_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a10fa455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 154 chunks from 12 documents.\n",
      "Generating embeddings for 154 chunks using Sentence Transformers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44538dc652ca4b14b07d40663f9983d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.0256778 , -0.0203816 , -0.02863   , ..., -0.00626529,\n",
       "        -0.01355996,  0.10974622],\n",
       "       [ 0.00353573, -0.03120735, -0.02974517, ...,  0.04245427,\n",
       "         0.02439736,  0.02422303],\n",
       "       [-0.03240658,  0.04424246,  0.01247747, ...,  0.04112267,\n",
       "         0.06839233,  0.01654245],\n",
       "       ...,\n",
       "       [-0.05692467,  0.0077112 , -0.00643335, ...,  0.06484738,\n",
       "        -0.02324439, -0.04590766],\n",
       "       [-0.06537127, -0.05666744,  0.00703504, ..., -0.07030727,\n",
       "        -0.02239653,  0.003152  ],\n",
       "       [ 0.07542328, -0.03661927,  0.06386682, ..., -0.02732012,\n",
       "         0.01738566, -0.05577966]], shape=(154, 384), dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmbedMaker = EmbeddingMaker()\n",
    "\n",
    "chunked_documents = EmbedMaker.chunk_documents(loaded_documents)\n",
    "embeddings = EmbedMaker.get_embeddings(chunked_documents)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04677e",
   "metadata": {},
   "source": [
    "## create Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5dded0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embeddings: 384\n",
      "Added 154 embeddings to the index.\n",
      "[INFO] Saved Faiss index and metadata to ./faiss_vectorstore\n",
      "[INFO] Loaded Faiss index and metadata from ./faiss_vectorstore\n",
      "[{'index': np.int64(143), 'distance': np.float32(1.039881), 'metadata': None}, {'index': np.int64(149), 'distance': np.float32(1.039881), 'metadata': None}, {'index': np.int64(87), 'distance': np.float32(1.5778074), 'metadata': None}]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Any, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class vectorstore_creator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            persistent_directory: str = \"./faiss_vectorstore\",\n",
    "            embed_model: str = \"all-MiniLM-L12-v2\",\n",
    "            chunk_size: int = 512,\n",
    "            chunk_overlap: int = 50,\n",
    "            batch_size: int = 16):\n",
    "        self.persistent_directory = persistent_directory\n",
    "        os.makedirs(self.persistent_directory, exist_ok=True)\n",
    "        self.embed_model = embed_model\n",
    "        self.metadata = []\n",
    "        self.chunk_size = chunk_size\n",
    "        self.index_type: Optional[str] = \"HNSW\"\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.batch_size = batch_size\n",
    "        self.model = SentenceTransformer(embed_model)\n",
    "        self.index = None\n",
    "        self.dimension = None\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def from_documents(self, documents: list[Any]):\n",
    "        \"\"\"Create a FAISS index from documents and embed_model.\"\"\"\n",
    "        embedder = EmbeddingMaker()\n",
    "        chunked_documents = embedder.chunk_documents(documents)\n",
    "        embeddings = embedder.get_embeddings(chunked_documents)\n",
    "        metadata = [{\"text\" : chunk.page_content} for chunk in chunked_documents]\n",
    "        self.add_embeddings(np.array(embeddings).astype(np.float32) , metadata)   \n",
    "        self.save()\n",
    "\n",
    "    def add_embeddings(self,embeddings: np.ndarray, metadata: list[Any]):\n",
    "        \"\"\"Add embeddings to the FAISS index.\"\"\"\n",
    "        if self.index is None:\n",
    "            self.dimension = embeddings.shape[1]\n",
    "            print(\"Dimension of embeddings:\", self.dimension)\n",
    "            self._initialize_index(self.dimension)\n",
    "        start = len(self.metadata)\n",
    "        ids = np.arange(start, start + len(embeddings)).astype(\"int64\")\n",
    "        self.index.add_with_ids(embeddings, ids)\n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        print(f\"Added {len(embeddings)} embeddings to the index.\")\n",
    "    \n",
    "    def _initialize_index(self, dimension: int):\n",
    "        if self.index_type == \"HNSW\":\n",
    "            self.internal_index = faiss.IndexHNSWFlat(self.dimension, 32)\n",
    "            self.internal_index.hnsw.efConstruction = 40\n",
    "            self.internal_index.hnsw.efSearch = 16\n",
    "        else:\n",
    "            self.internal_index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index = faiss.IndexIDMap(self.internal_index)\n",
    "        \n",
    "    def save(self):\n",
    "        faiss_path = os.path.join(self.persistent_directory, \"faiss.index\")\n",
    "        meta_path = os.path.join(self.persistent_directory, \"metadata.pkl\")\n",
    "        faiss.write_index(self.index, faiss_path)\n",
    "        with open(meta_path, \"wb\") as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "        print(f\"[INFO] Saved Faiss index and metadata to {self.persistent_directory}\")\n",
    "\n",
    "    def load(self):\n",
    "        faiss_path = os.path.join(self.persistent_directory, \"faiss.index\")\n",
    "        meta_path = os.path.join(self.persistent_directory, \"metadata.pkl\")\n",
    "        self.index = faiss.read_index(faiss_path)\n",
    "        with open(meta_path, \"rb\") as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        print(f\"[INFO] Loaded Faiss index and metadata from {self.persistent_directory}\")\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 5):\n",
    "        \"\"\"Search the FAISS index for similar embeddings.\"\"\"\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        results = []\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            metadata = self.metadata[idx] if idx < len(self.metadata) else None\n",
    "            results.append({\"index\": idx, \"distance\": dist,\"metadata\": metadata})\n",
    "        return results\n",
    "        \n",
    "    def query(self,query: str, top_k: int=5):\n",
    "        \"\"\"Get embedding fro the query and search the query\"\"\"\n",
    "        query_embedding = self.model.encode([query]).astype(np.float32)\n",
    "        results = self.search(query_embedding, top_k)\n",
    "        return results\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    vector_store = vectorstore_creator(\"./faiss_vectorstore\")\n",
    "    metadata = [{\"text\": doc.page_content} for doc in loaded_documents]\n",
    "    vector_store.add_embeddings(embeddings, metadata)\n",
    "    vector_store.save()\n",
    "    vector_store.load()\n",
    "    print(vector_store.query(\"Real-time CCTV-based PPE Detection\",top_k=3))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10c02a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': np.int64(143), 'distance': np.float32(1.039881), 'metadata': None}, {'index': np.int64(149), 'distance': np.float32(1.039881), 'metadata': None}, {'index': np.int64(87), 'distance': np.float32(1.5778074), 'metadata': None}]\n"
     ]
    }
   ],
   "source": [
    "print(vector_store.query(\"Real-time CCTV-based PPE Detection\",top_k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57014fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded Faiss index and metadata from ./faiss_vectorstore\n"
     ]
    },
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGroqError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     rag_search \u001b[38;5;241m=\u001b[39m \u001b[43mRAGSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Ppe detection?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m     summary \u001b[38;5;241m=\u001b[39m rag_search\u001b[38;5;241m.\u001b[39msearch_and_summarize(query, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m, in \u001b[0;36mRAGSearch.__init__\u001b[0;34m(self, persistent_directory, embed_model, llm_model)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     15\u001b[0m groq_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mChatGroq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroq_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroq_api_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Groq LLM initialized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/myenv/lib/python3.10/site-packages/langchain_core/load/serializable.py:116\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/myenv/lib/python3.10/site-packages/langchain_groq/chat_models.py:477\u001b[0m, in \u001b[0;36mChatGroq.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m sync_specific: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient:\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mgroq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n\u001b[1;32m    481\u001b[0m     async_specific: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client}\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/myenv/lib/python3.10/site-packages/groq/_client.py:79\u001b[0m, in \u001b[0;36mGroq.__init__\u001b[0;34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     77\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mGroqError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class RAGSearch:\n",
    "    def __init__(self,persistent_directory : str = \"./faiss_vectorstore\", embed_model: str = \"all-MiniLM-L12-v2\", llm_model: str = \"gemma2-9b-it\"):\n",
    "        self.vectorstore = vectorstore_creator(persistent_directory, embed_model)\n",
    "\n",
    "        faiss_path = os.path.join(persistent_directory, \"faiss.index\")\n",
    "        meta_path = os.path.join(persistent_directory, \"metadata.pkl\")\n",
    "\n",
    "        if not (os.path.exists(faiss_path) and os.path.exists(meta_path)):\n",
    "            docs = document_loader(\"./rag_data\")\n",
    "            self.vectorstore.from_documents(docs)\n",
    "        else:\n",
    "            self.vectorstore.load()\n",
    "        groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        self.llm = ChatGroq(groq_api_key=groq_api_key, model_name=llm_model)\n",
    "        print(f\"[INFO] Groq LLM initialized: {llm_model}\")\n",
    "    \n",
    "\n",
    "    def search_and_summarize(self, query: str, top_k: int = 5) -> str:\n",
    "        results = self.vectorstore.query(query, top_k=top_k)\n",
    "        texts = [r[\"metadata\"].get(\"text\", \"\") for r in results if r[\"metadata\"]]\n",
    "        context = \"\\n\\n\".join(texts)\n",
    "        if not context:\n",
    "            return \"No relevant documents found.\"\n",
    "        prompt = f\"\"\"Summarize the following context for the query: '{query}'\\n\\nContext:\\n{context}\\n\\nSummary:\"\"\"\n",
    "        response = self.llm.invoke([prompt])\n",
    "        return response.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rag_search = RAGSearch()\n",
    "    query = \"What is Ppe detection?\"\n",
    "    summary = rag_search.search_and_summarize(query, top_k=3)\n",
    "    print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbcd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://drive.google.com/file/d/1NTa_RiSHFz2tgglo1MO2Uzlxz3-sMZRY/view?usp=sharing\n",
    "\n",
    "https://drive.google.com/file/d/1S-Nw1WSp_GngcPFZt354ebS8VPBD-DI9/view?usp=sharing\n",
    "\n",
    "\n",
    "\n",
    "https://drive.google.com/file/d/1S-Nw1WSp_GngcPFZt354ebS8VPBD-DI9/view?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
